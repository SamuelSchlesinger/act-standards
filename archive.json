{
  "magic": "E!vIA5L86J2I",
  "timestamp": "2025-08-19T00:05:35.234927+00:00",
  "repo": "SamuelSchlesinger/draft-act",
  "labels": [
    {
      "name": "bug",
      "description": "Something isn't working",
      "color": "d73a4a"
    },
    {
      "name": "documentation",
      "description": "Improvements or additions to documentation",
      "color": "0075ca"
    },
    {
      "name": "duplicate",
      "description": "This issue or pull request already exists",
      "color": "cfd3d7"
    },
    {
      "name": "enhancement",
      "description": "New feature or request",
      "color": "a2eeef"
    },
    {
      "name": "good first issue",
      "description": "Good for newcomers",
      "color": "7057ff"
    },
    {
      "name": "help wanted",
      "description": "Extra attention is needed",
      "color": "008672"
    },
    {
      "name": "invalid",
      "description": "This doesn't seem right",
      "color": "e4e669"
    },
    {
      "name": "question",
      "description": "Further information is requested",
      "color": "d876e3"
    },
    {
      "name": "wontfix",
      "description": "This will not be worked on",
      "color": "ffffff"
    }
  ],
  "issues": [
    {
      "number": 1,
      "id": "I_kwDOPc24Mc7FnraH",
      "title": "Use CFRG boiler plate for prime order groups",
      "url": "https://github.com/SamuelSchlesinger/draft-act/issues/1",
      "state": "OPEN",
      "author": "cjpatton",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "Ristretto255 will work great for many, but some will likely need NIST curves like P-256 or P-384. The simplest thing would be to use a generic prime order group. Many CFRG drafts use the same boiler plate API for this, [as does ARC](https://chris-wood.github.io/draft-arc/draft-yun-cfrg-arc.html#name-prime-order-group). I suggest adopting this API here as well. Not only will this make it easier to adopt new curves, it also aligns the draft closer to what IETF WGs are used to.",
      "createdAt": "2025-08-12T19:08:47Z",
      "updatedAt": "2025-08-12T19:09:35Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 2,
      "id": "I_kwDOPc24Mc7Fn6Gj",
      "title": "Replace BLAKE3 with SHAKE128 or TurboSHAKE128",
      "url": "https://github.com/SamuelSchlesinger/draft-act/issues/2",
      "state": "OPEN",
      "author": "cjpatton",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "BLAKE3 is not \"standard\" in the sense that no standards organizations has endorsed a specification of it (at least not to my knowledge). This may be a deal breaker for some deployment scenarios.\n\nBoth SHAKE128 as specified in [FIPS 202](https://csrc.nist.gov/pubs/fips/202/final) and TurboSHAKE128 as specified in [draft-irtf-cfrg-kangaroo-twelve](https://datatracker.ietf.org/doc/draft-irtf-cfrg-kangarootwelve/) are viable options. As of writing, [draft-irtf-cfrg-fiat-shamir](https://www.ietf.org/archive/id/draft-irtf-cfrg-fiat-shamir-00) uses SHAKE128.\n\nAn alternative would be to try and get [BLAKE3 at CFRG](https://mailarchive.ietf.org/arch/msg/cfrg/ZSbnJbI1JUkf_VHO4A97TYFFmTQ/). However, my preference would be to not make this a blocker unless there is a compelling reason, e.g., we need the performance benefit.",
      "createdAt": "2025-08-12T19:23:23Z",
      "updatedAt": "2025-08-13T14:31:19Z",
      "closedAt": null,
      "comments": [
        {
          "author": "SamuelSchlesinger",
          "authorAssociation": "OWNER",
          "body": "We should benchmark this change in the prototype before attempting to get BLAKE3 at CFRG. Either way, making the scheme modular with a hash function as an input seems reasonable as well.",
          "createdAt": "2025-08-13T09:05:56Z",
          "updatedAt": "2025-08-13T09:05:56Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "NONE",
          "body": "Ah, that's a good idea in any case!",
          "createdAt": "2025-08-13T14:31:19Z",
          "updatedAt": "2025-08-13T14:31:19Z"
        }
      ]
    },
    {
      "number": 3,
      "id": "I_kwDOPc24Mc7Fn8Mu",
      "title": "Adopt RFC 9380 for hashing to Ristretto255",
      "url": "https://github.com/SamuelSchlesinger/draft-act/issues/3",
      "state": "OPEN",
      "author": "cjpatton",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "As specified in [Appendix B](https://www.rfc-editor.org/rfc/rfc9380.html#appendix-B). The same RFC specifies algorithms for NIST curves as well (#1).",
      "createdAt": "2025-08-12T19:25:48Z",
      "updatedAt": "2025-08-12T19:25:48Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 4,
      "id": "I_kwDOPc24Mc7Fn_g-",
      "title": "Align terminology with draft-yun-cfrg-arc as much as possible",
      "url": "https://github.com/SamuelSchlesinger/draft-act/issues/4",
      "state": "OPEN",
      "author": "cjpatton",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "[ARC](datatracker.ietf.org/doc/draft-yun-cfrg-arc/) and ACT are useful for similar, though not identical use cases. I would expect that many who want to adopt ACT would also want to adopt ARC. In order to improve communication, it would be useful to align terminology between the two drafts wherever possible.",
      "createdAt": "2025-08-12T19:29:19Z",
      "updatedAt": "2025-08-13T09:04:12Z",
      "closedAt": null,
      "comments": [
        {
          "author": "SamuelSchlesinger",
          "authorAssociation": "OWNER",
          "body": "Sounds great insofar as they have shared concepts or primitives. For instance, once we implement the range proof in ARC, we should align on that.",
          "createdAt": "2025-08-13T09:04:04Z",
          "updatedAt": "2025-08-13T09:04:12Z"
        }
      ]
    },
    {
      "number": 5,
      "id": "I_kwDOPc24Mc7FoELi",
      "title": "Adopt draft-irtf-cfrg-fiat-shamir and draft-irtf-cfrg-sigma-protocols",
      "url": "https://github.com/SamuelSchlesinger/draft-act/issues/5",
      "state": "OPEN",
      "author": "cjpatton",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "CFRG recently signed up to work out [Sigma protocols](https://datatracker.ietf.org/doc/draft-irtf-cfrg-sigma-protocols/) and the [Fiat-Shamir transform](https://datatracker.ietf.org/doc/draft-irtf-cfrg-fiat-shamir/). To the extent that is possible, ACT should be built on top of them so that there is less for the draft to specify. Note that it may be necessary to add support for $\\vee$-proofs.",
      "createdAt": "2025-08-12T19:34:24Z",
      "updatedAt": "2025-08-12T23:47:34Z",
      "closedAt": null,
      "comments": [
        {
          "author": "SamuelSchlesinger",
          "authorAssociation": "OWNER",
          "body": "Totally agree with this, though I don't have the bandwidth to prioritize this myself at this point. I will happily review text to this end.",
          "createdAt": "2025-08-12T23:47:34Z",
          "updatedAt": "2025-08-12T23:47:34Z"
        }
      ]
    },
    {
      "number": 7,
      "id": "I_kwDOPc24Mc7GlytP",
      "title": "Use case for request context: expiry",
      "url": "https://github.com/SamuelSchlesinger/draft-act/issues/7",
      "state": "OPEN",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "ACT credentials should expire, for two reasons:\n\n1. Security: The server may want to rate-limit clients over time, say, to $c$ requests/day, where $c$ is the number of credits for each credential issued. Unless credentials expire, issuing one credential/day per client would not be sufficient to enforce the rate limit, since the client could hoard credentials issued over multiple days.\n\n2. Operational: Preventing double spends requires storing all nullifiers accepted for a given server key. This means the cost of the double spending check increases over time as the number of stored nullifiers increases. Once a credential expires, it's no longer necessary to track its nullifiers.\n\nThe draft currently envisions implementing expiry by frequently rotating the server's key. Time would be divided into discrete epochs, say 00:00-23:59 UTC, and each epoch would have its own server key. Clients would need to be configured out-of-band with the corresponding public key for each epoch.\n\nThis imposes an operational burden that may be untenable for some deployments. For example, suppose the clients are operated by company X and credentials are issued by (and presented to) company Y. Suppose further that X requests issuance from Y on behalf of its clients. (This prevents Y from being able to fingerprint clients. It also makes it possible to separate issuance from attestation of who is a \"legitimate\" client, which is helpful in many scenarios. See [RFC 9576](https://datatracker.ietf.org/doc/rfc9576/).)\n\nTo implement this feature, Y would expose an API to X for pulling epoch public keys; X would periodically ping this API and push keys down to its clients. How \"doable\" this is in practice depends on the length of the epoch window. Rotating every day may not be so bad, but rotating every hour would require a lot of coordination.\n\nOne way around this would be to support something like ARC's [request context](https://chris-wood.github.io/draft-arc/draft-yun-cfrg-arc.html#section-4.2.1). Roughly speaking, the request context is some string that the client and server must agree on in order for issuance or presentation to succeed. The context might encode the epoch in which the credential was issued. During presentation, the server would use the _current epoch_ as the request context, thereby rejecting credentials issued too far in the past.\n",
      "createdAt": "2025-08-18T19:09:59Z",
      "updatedAt": "2025-08-18T20:15:29Z",
      "closedAt": null,
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "A few thoughts on how we might implement request context.\n\n1. We're already deriving the generators $h_1, h_2, h_3$ (using the notation of the [write up](https://github.com/SamuelSchlesinger/anonymous-credit-tokens/blob/main/docs/design.pdf)) by hashing fixed strings. What about simply incorporating the request context into the hash? \n\n2. The BBS MAC the client gets from the server is over a commitment to a vector that includes the current budget $c$ and the next nullifier $k$. We could either this vector with an additional scalar $w$ that includes the hash of the request context so that the MAC becomes:\n\n    $$A = \\left(g \\cdot h_1^c \\cdot h_2^k \\cdot h_3^w \\cdot  h_4^r \\right)^{1/(x+e)}$$\n\n3. We could tack on a fresh commitment to $w$ to the signed message:\n\n    $$A = \\left(g \\cdot (h_1^c \\cdot h_2^k \\cdot h_3^{r_0}) \\cdot (h_4^w \\cdot h_5^{r_1})\\right)^{1/(x+e)}$$\n\n    where $r_0, r_1$ are random scalars.\n\nThe first one seems like the least invasive, but I'm not sure how it would impact security analysis. In terms of performance, it would be more expensive, since we have to compute the generators at issuance/presentation time rather than at setup time. @armfazh ball parks the cost of each hash to curve to be about 1/4 the cost of scalar multiplication.\n\nThe second one seems pretty natural to me, but I'm not sure how this would impact the ZKPs. Likewise for the third option. Both would cost a bit more in performance.",
          "createdAt": "2025-08-18T19:24:56Z",
          "updatedAt": "2025-08-18T20:15:29Z"
        }
      ]
    },
    {
      "number": 8,
      "id": "I_kwDOPc24Mc7GmHPU",
      "title": "Use case for presentation context: bind presentation to origin",
      "url": "https://github.com/SamuelSchlesinger/draft-act/issues/8",
      "state": "OPEN",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "ACT could be used to rate limit HTTP requests from clients to an HTTP server. In many such cases, the server will operate multiple \"origins\". For example, the same content delivery network might serve HTTP requests for multiple websites.\n\nIn this deployment scenario, it's useful to be able to enforce the rate limit on a per-origin basis. That is, when the client is issued a token with $c$ credits, it gets to make at most $c$ requests to each origin operated by the server.\n\nI see two ways to do this with ACT as it is, neither of which is ideal:\n\n1. The server could issue $c\\cdot N$ credits, where $N$ is the number of origins. However, the client could just spend all of its token at one origin and thereby bypass the per-origin rate-limit.\n\n2. The server could use separate keys for each origin, but this would require the client to reveal to the issuer which origin it wants to reach.\n\nWhat we really want is something like ARC's [presentation context](https://chris-wood.github.io/draft-arc/draft-yun-cfrg-arc.html#section-4.3-3.1.1) string. For our use case, this string would encode the origin, e.g., $\\mathit{ctx} = \\texttt{example.com}$. It would be great to provide ACT with a similar feature.",
      "createdAt": "2025-08-18T19:39:55Z",
      "updatedAt": "2025-08-18T19:54:54Z",
      "closedAt": null,
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "ARC works as follows. For each presentation context $\\mathit{ctx}$, the client initializes a fresh \"presentation\" state. For each presentation, it chooses a random nonce $n$ from $[1..c]$ and computes a tag $t = \\mathsf{PRF}_k(n, \\mathit{ctx})$, where $k$ is a scalar it committed to in its credential. The server tracks these $(n, t)$ pairs on a per-origin basis.\n\nCan we do something similar for ACT? Here's an initial idea that doesn't work: Instead of revealing the nullifier $k$ directly, we might instead reveal $\\mathsf{PRF}_k(\\mathit{ctx})$.\n\n@meyira pointed the following attack: Given credential state $S$ with $c$ spend, a malicious client could present $S$ to  a number of origins, then present each of the re-issued credentials to some target origin. We therefore need some way of binding the re-issued credential to the origin so that it can only be presented there.\n\nAlso, we only know how to instantiate $\\mathsf{PRF}$ with classical assumptions (cf. [2024/1552](https://eprint.iacr.org/2024/1552), Theorem 8.7). We therefore would want to make this feature optional so that not all deployments need to sacrifice PQ privacy.\n",
          "createdAt": "2025-08-18T19:54:24Z",
          "updatedAt": "2025-08-18T19:54:54Z"
        }
      ]
    }
  ],
  "pulls": [
    {
      "number": 6,
      "id": "PR_kwDOPc24Mc6j4QmL",
      "title": "Update references to github repo",
      "url": "https://github.com/SamuelSchlesinger/draft-act/pull/6",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The new name is \"draft-act\".",
      "createdAt": "2025-08-15T18:49:41Z",
      "updatedAt": "2025-08-15T18:58:08Z",
      "baseRepository": "SamuelSchlesinger/draft-act",
      "baseRefName": "main",
      "baseRefOid": "837050a15f183bba998e3a95badf53a101ec6fef",
      "headRepository": "SamuelSchlesinger/draft-act",
      "headRefName": "cjpatton/update-repo-name",
      "headRefOid": "f2458b30f77a253c4203034475360549d6a6791e",
      "closedAt": "2025-08-15T18:58:07Z",
      "mergedAt": "2025-08-15T18:58:07Z",
      "mergedBy": "SamuelSchlesinger",
      "mergeCommit": {
        "oid": "a71be413158b134b432228d028719fd56543663f"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOPc24Mc66QYH0",
          "commit": {
            "abbreviatedOid": "f2458b3"
          },
          "author": "SamuelSchlesinger",
          "authorAssociation": "OWNER",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2025-08-15T18:58:03Z",
          "updatedAt": "2025-08-15T18:58:03Z",
          "comments": []
        }
      ]
    }
  ]
}